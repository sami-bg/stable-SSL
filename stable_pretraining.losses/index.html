
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>stable_pretraining.losses package &#8212; stable-pretraining 0.1.dev1+g5059ec3d9 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=721ff2ef"></script>
    <script src="../_static/doctools.js?v=fd6eb6e6"></script>
    <script src="../_static/sphinx_highlight.js?v=6ffebe34"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'stable_pretraining.losses';</script>
    <link rel="index" title="Index" href="../genindex/" />
    <link rel="search" title="Search" href="../search/" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search/"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../">
  
  
  
  
  
  
    <p class="title logo__title">stable-pretraining 0.1.dev1+g5059ec3d9 documentation</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../">stable-pretraining</a></li>
<li class="toctree-l1"><a class="reference internal" href="../releases/">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing/">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../bibliography/">Bibliography</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">API</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../api/backbone/">stable_pretraining.backbone</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.backbone.MLP/">MLP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.backbone.Resnet9/">Resnet9</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.backbone.ConvMixer/">ConvMixer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.backbone.from_timm/">from_timm</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.backbone.from_torchvision/">from_torchvision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.backbone.set_embedding_dim/">set_embedding_dim</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.backbone.TeacherStudentWrapper/">TeacherStudentWrapper</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.backbone.EvalOnly/">EvalOnly</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.backbone.mae/">mae</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../api/data/">stable_pretraining.data</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.DataModule/">DataModule</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.Collator/">Collator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.Dataset/">Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.FromTorchDataset/">FromTorchDataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.HFDataset/">HFDataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.Subset/">Subset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.GMM/">GMM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.MinariStepsDataset/">MinariStepsDataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.MinariEpisodeDataset/">MinariEpisodeDataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.swiss_roll/">swiss_roll</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.generate_perlin_noise_2d/">generate_perlin_noise_2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.perlin_noise_3d/">perlin_noise_3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.Categorical/">Categorical</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.ExponentialMixtureNoiseModel/">ExponentialMixtureNoiseModel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.ExponentialNormalNoiseModel/">ExponentialNormalNoiseModel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.RepeatedRandomSampler/">RepeatedRandomSampler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.SupervisedBatchSampler/">SupervisedBatchSampler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.RandomBatchSampler/">RandomBatchSampler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.fold_views/">fold_views</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.random_split/">random_split</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.download/">download</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.bulk_download/">bulk_download</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.transforms/">stable_pretraining.data.transforms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.dataset_stats/">stable_pretraining.data.dataset_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.synthetic_data/">stable_pretraining.data.synthetic_data</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../api/forward/">stable_pretraining.forward</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../api/losses/">stable_pretraining.losses</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.losses.NTXEntLoss/">NTXEntLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.losses.NegativeCosineSimilarity/">NegativeCosineSimilarity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.losses.VICRegLoss/">VICRegLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.losses.BarlowTwinsLoss/">BarlowTwinsLoss</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../api/manager/">stable_pretraining.manager</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.manager.Manager/">Manager</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../api/module/">stable_pretraining.module</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.module.Module/">Module</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../api/monitors/">stable_pretraining.callbacks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.callbacks.OnlineProbe/">OnlineProbe</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.callbacks.OnlineKNN/">OnlineKNN</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.callbacks.OnlineWriter/">OnlineWriter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.callbacks.RankMe/">RankMe</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.callbacks.LiDAR/">LiDAR</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.callbacks.EarlyStopping/">EarlyStopping</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.callbacks.TrainerInfo/">TrainerInfo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.callbacks.LoggingCallback/">LoggingCallback</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.callbacks.ModuleSummary/">ModuleSummary</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.callbacks.SklearnCheckpoint/">SklearnCheckpoint</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.callbacks.ImageRetrieval/">ImageRetrieval</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../api/optim/">stable_pretraining.optim</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.optim.LARS/">LARS</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.optim.CosineDecayer/">CosineDecayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.optim.LinearWarmup/">LinearWarmup</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.optim.LinearWarmupCosineAnnealing/">LinearWarmupCosineAnnealing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.optim.LinearWarmupCyclicAnnealing/">LinearWarmupCyclicAnnealing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.optim.LinearWarmupThreeStepsAnnealing/">LinearWarmupThreeStepsAnnealing</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/stable_pretraining.losses.rst" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.rst</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>stable_pretraining.losses package</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#submodules">Submodules</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-stable_pretraining.losses.dino">stable_pretraining.losses.dino module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.dino.DINOv1Loss"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.dino.DINOv1Loss.apply_center_update"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss.apply_center_update()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.dino.DINOv1Loss.forward"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.dino.DINOv1Loss.sinkhorn_knopp_teacher"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss.sinkhorn_knopp_teacher()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.dino.DINOv1Loss.softmax_center_teacher"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss.softmax_center_teacher()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.dino.DINOv1Loss.update_center"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss.update_center()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.dino.DINOv2Loss"><code class="docutils literal notranslate"><span class="pre">DINOv2Loss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.dino.DINOv2Loss.forward"><code class="docutils literal notranslate"><span class="pre">DINOv2Loss.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.dino.cross_entropy_loss"><code class="docutils literal notranslate"><span class="pre">cross_entropy_loss()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.dino.iBOTPatchLoss"><code class="docutils literal notranslate"><span class="pre">iBOTPatchLoss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.dino.iBOTPatchLoss.forward"><code class="docutils literal notranslate"><span class="pre">iBOTPatchLoss.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.dino.iBOTPatchLoss.sinkhorn_knopp_teacher"><code class="docutils literal notranslate"><span class="pre">iBOTPatchLoss.sinkhorn_knopp_teacher()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-stable_pretraining.losses.joint_embedding">stable_pretraining.losses.joint_embedding module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.joint_embedding.BYOLLoss"><code class="docutils literal notranslate"><span class="pre">BYOLLoss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.joint_embedding.BYOLLoss.forward"><code class="docutils literal notranslate"><span class="pre">BYOLLoss.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.joint_embedding.BarlowTwinsLoss"><code class="docutils literal notranslate"><span class="pre">BarlowTwinsLoss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.joint_embedding.BarlowTwinsLoss.forward"><code class="docutils literal notranslate"><span class="pre">BarlowTwinsLoss.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.joint_embedding.InfoNCELoss"><code class="docutils literal notranslate"><span class="pre">InfoNCELoss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.joint_embedding.InfoNCELoss.forward"><code class="docutils literal notranslate"><span class="pre">InfoNCELoss.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.joint_embedding.NTXEntLoss"><code class="docutils literal notranslate"><span class="pre">NTXEntLoss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.joint_embedding.NTXEntLoss.forward"><code class="docutils literal notranslate"><span class="pre">NTXEntLoss.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.joint_embedding.SwAVLoss"><code class="docutils literal notranslate"><span class="pre">SwAVLoss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.joint_embedding.SwAVLoss.forward"><code class="docutils literal notranslate"><span class="pre">SwAVLoss.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.joint_embedding.SwAVLoss.sinkhorn"><code class="docutils literal notranslate"><span class="pre">SwAVLoss.sinkhorn()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.joint_embedding.SwAVLoss.swapped_prediction"><code class="docutils literal notranslate"><span class="pre">SwAVLoss.swapped_prediction()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.joint_embedding.VICRegLoss"><code class="docutils literal notranslate"><span class="pre">VICRegLoss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.joint_embedding.VICRegLoss.forward"><code class="docutils literal notranslate"><span class="pre">VICRegLoss.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-stable_pretraining.losses.multimodal">stable_pretraining.losses.multimodal module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.multimodal.CLIPLoss"><code class="docutils literal notranslate"><span class="pre">CLIPLoss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.multimodal.CLIPLoss.forward"><code class="docutils literal notranslate"><span class="pre">CLIPLoss.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-stable_pretraining.losses.reconstruction">stable_pretraining.losses.reconstruction module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.reconstruction.mae"><code class="docutils literal notranslate"><span class="pre">mae()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-stable_pretraining.losses.utils">stable_pretraining.losses.utils module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.utils.NegativeCosineSimilarity"><code class="docutils literal notranslate"><span class="pre">NegativeCosineSimilarity</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.utils.NegativeCosineSimilarity.forward"><code class="docutils literal notranslate"><span class="pre">NegativeCosineSimilarity.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.utils.off_diagonal"><code class="docutils literal notranslate"><span class="pre">off_diagonal()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.utils.sinkhorn_knopp"><code class="docutils literal notranslate"><span class="pre">sinkhorn_knopp()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-stable_pretraining.losses">Module contents</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.BYOLLoss"><code class="docutils literal notranslate"><span class="pre">BYOLLoss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.BYOLLoss.forward"><code class="docutils literal notranslate"><span class="pre">BYOLLoss.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.BarlowTwinsLoss"><code class="docutils literal notranslate"><span class="pre">BarlowTwinsLoss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.BarlowTwinsLoss.forward"><code class="docutils literal notranslate"><span class="pre">BarlowTwinsLoss.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.CLIPLoss"><code class="docutils literal notranslate"><span class="pre">CLIPLoss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.CLIPLoss.forward"><code class="docutils literal notranslate"><span class="pre">CLIPLoss.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.DINOv1Loss"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.DINOv1Loss.apply_center_update"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss.apply_center_update()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.DINOv1Loss.forward"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.DINOv1Loss.sinkhorn_knopp_teacher"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss.sinkhorn_knopp_teacher()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.DINOv1Loss.softmax_center_teacher"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss.softmax_center_teacher()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.DINOv1Loss.update_center"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss.update_center()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.DINOv2Loss"><code class="docutils literal notranslate"><span class="pre">DINOv2Loss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.DINOv2Loss.forward"><code class="docutils literal notranslate"><span class="pre">DINOv2Loss.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.NTXEntLoss"><code class="docutils literal notranslate"><span class="pre">NTXEntLoss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.NTXEntLoss.forward"><code class="docutils literal notranslate"><span class="pre">NTXEntLoss.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.NegativeCosineSimilarity"><code class="docutils literal notranslate"><span class="pre">NegativeCosineSimilarity</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.NegativeCosineSimilarity.forward"><code class="docutils literal notranslate"><span class="pre">NegativeCosineSimilarity.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.VICRegLoss"><code class="docutils literal notranslate"><span class="pre">VICRegLoss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.VICRegLoss.forward"><code class="docutils literal notranslate"><span class="pre">VICRegLoss.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.iBOTPatchLoss"><code class="docutils literal notranslate"><span class="pre">iBOTPatchLoss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.iBOTPatchLoss.forward"><code class="docutils literal notranslate"><span class="pre">iBOTPatchLoss.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.iBOTPatchLoss.sinkhorn_knopp_teacher"><code class="docutils literal notranslate"><span class="pre">iBOTPatchLoss.sinkhorn_knopp_teacher()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.mae"><code class="docutils literal notranslate"><span class="pre">mae()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.off_diagonal"><code class="docutils literal notranslate"><span class="pre">off_diagonal()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.sinkhorn_knopp"><code class="docutils literal notranslate"><span class="pre">sinkhorn_knopp()</span></code></a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="stable-pretraining-losses-package">
<h1>stable_pretraining.losses package<a class="headerlink" href="#stable-pretraining-losses-package" title="Link to this heading">#</a></h1>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Link to this heading">#</a></h2>
</section>
<section id="module-stable_pretraining.losses.dino">
<span id="stable-pretraining-losses-dino-module"></span><h2>stable_pretraining.losses.dino module<a class="headerlink" href="#module-stable_pretraining.losses.dino" title="Link to this heading">#</a></h2>
<p>DINO self-distillation losses.</p>
<p>This module contains losses for DINO-style self-distillation including:
- DINOLoss: CLS token distillation
- iBOTPatchLoss: Masked patch prediction</p>
<p>Reference: DINOv2/v3 papers and <a class="github reference external" href="https://github.com/facebookresearch/dinov3">facebookresearch/dinov3</a></p>
<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.losses.dino.DINOv1Loss">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">stable_pretraining.losses.dino.</span></span><span class="sig-name descname"><span class="pre">DINOv1Loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">temperature_student</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">center_momentum</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.9</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/dino/#DINOv1Loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.dino.DINOv1Loss" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>DINOv1 loss for self-distillation with cross-entropy <span id="id1">[<a class="reference internal" href="../bibliography/#id9" title="Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, 9650–9660. 2021.">Caron <em>et al.</em>, 2021</a>]</span>.</p>
<p>This loss computes cross-entropy between teacher and student logits after applying
temperature scaling and normalization. The teacher uses either classical centering or
Sinkhorn-Knopp normalization to prevent mode collapse.</p>
<dl>
<dt>Usage:</dt><dd><p><a href="#id2"><span class="problematic" id="id3">``</span></a><a href="#id4"><span class="problematic" id="id5">`</span></a>python
dino_loss = DINOv1Loss()</p>
<p># Get logits from prototype layer
student_logits = prototype_layer(student_embeddings)  # [n_views, B, out_dim]
teacher_logits = prototype_layer(teacher_embeddings)  # [n_views, B, out_dim]</p>
<p># Approach 1: Classical centering (recommended, faster)
teacher_probs = dino_loss.softmax_center_teacher(teacher_logits, temp=0.04)
loss = dino_loss(student_logits, teacher_probs)
dino_loss.update_center(teacher_logits)  # Queue async center update</p>
<p># Approach 2: Sinkhorn-Knopp (more principled, slower, no centering needed)
n_views, batch_size, _ = teacher_logits.shape
num_samples = n_views * batch_size  # Total samples across views
teacher_probs = dino_loss.sinkhorn_knopp_teacher(</p>
<blockquote>
<div><p>teacher_logits, temp=0.04, num_samples=num_samples</p>
</div></blockquote>
<p>)
loss = dino_loss(student_logits, teacher_probs)
# No update_center() needed for Sinkhorn-Knopp!
<a href="#id6"><span class="problematic" id="id7">``</span></a><a href="#id8"><span class="problematic" id="id9">`</span></a></p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>temperature_student</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a>) – Temperature for student softmax. Default is 0.1.</p></li>
<li><p><strong>center_momentum</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a>) – EMA momentum for center update. Default is 0.9.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.losses.dino.DINOv1Loss.apply_center_update">
<span class="sig-name descname"><span class="pre">apply_center_update</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/dino/#DINOv1Loss.apply_center_update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.dino.DINOv1Loss.apply_center_update" title="Link to this definition">#</a></dt>
<dd><p>Apply the queued center update with EMA.</p>
<p><strong>FOR CLASSICAL CENTERING APPROACH ONLY. NOT NEEDED FOR SINKHORN-KNOPP.</strong></p>
<p>Waits for async all-reduce to complete and updates self.center with EMA.
Automatically called by softmax_center_teacher() if update_centers=True.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.losses.dino.DINOv1Loss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">student_logits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">teacher_probs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></span><a class="reference internal" href="../_modules/stable_pretraining/losses/dino/#DINOv1Loss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.dino.DINOv1Loss.forward" title="Link to this definition">#</a></dt>
<dd><p>Compute DINO cross-entropy loss.</p>
<p>This is a pure loss computation with no side effects (no centering, no updates).
Teacher probabilities should be pre-processed with softmax_center_teacher() or
sinkhorn_knopp_teacher(). Center updates should be done separately with update_center().</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>student_logits</strong> – Student logits [n_views, batch_size, out_dim]</p></li>
<li><p><strong>teacher_probs</strong> – Teacher probabilities (already normalized) [n_views, batch_size, out_dim]</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Scalar DINO loss value (cross-entropy averaged over view pairs, excluding diagonal)</p>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>student_logits: (S, B, K) where S = student views, B = batch size, K = out_dim</p></li>
<li><p>teacher_probs: (T, B, K) where T = teacher views</p></li>
<li><p>output: scalar</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.losses.dino.DINOv1Loss.sinkhorn_knopp_teacher">
<span class="sig-name descname"><span class="pre">sinkhorn_knopp_teacher</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">teacher_logits</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">teacher_temp</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iterations</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/dino/#DINOv1Loss.sinkhorn_knopp_teacher"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.dino.DINOv1Loss.sinkhorn_knopp_teacher" title="Link to this definition">#</a></dt>
<dd><p>Apply Sinkhorn-Knopp optimal transport normalization to teacher logits.</p>
<p><strong>FOR SINKHORN-KNOPP APPROACH ONLY. DOES NOT USE CENTER.</strong></p>
<p>This method applies sinkhorn-knopp to enforce exact uniform distribution across
prototypes without using centering. More principled than centering but more expensive.
Used in SwAV and DINOv3 for better theoretical guarantees.</p>
<p>Note: When using Sinkhorn-Knopp, you do NOT need to call update_center() or
apply_center_update() since centering is not used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>teacher_logits</strong> – Teacher logits [<a href="#id10"><span class="problematic" id="id11">*</span></a>, out_dim]. Can be any shape as long as last dim is out_dim.
Common shapes: [batch, out_dim] or [n_views, batch, out_dim]</p></li>
<li><p><strong>teacher_temp</strong> – Temperature for softmax</p></li>
<li><p><strong>num_samples</strong> – Total number of samples across all GPUs (int or tensor).
If None, inferred from shape assuming [batch, out_dim] format.
For multi-view [n_views, batch, out_dim], pass n_views * batch explicitly.</p></li>
<li><p><strong>n_iterations</strong> – Number of Sinkhorn iterations (default: 3)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Teacher probabilities [same shape as input] with uniform prototype distribution</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.losses.dino.DINOv1Loss.softmax_center_teacher">
<span class="sig-name descname"><span class="pre">softmax_center_teacher</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">teacher_logits</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">teacher_temp</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">update_centers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/dino/#DINOv1Loss.softmax_center_teacher"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.dino.DINOv1Loss.softmax_center_teacher" title="Link to this definition">#</a></dt>
<dd><p>Apply classical centering and sharpening to teacher logits.</p>
<p><strong>FOR CLASSICAL CENTERING APPROACH ONLY. NOT NEEDED FOR SINKHORN-KNOPP.</strong></p>
<p>This method subtracts the center (EMA of batch means) from teacher logits before
applying softmax. This prevents mode collapse by ensuring balanced prototype usage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>teacher_logits</strong> – Teacher logits [<a href="#id12"><span class="problematic" id="id13">*</span></a>, out_dim]</p></li>
<li><p><strong>teacher_temp</strong> – Temperature for teacher softmax</p></li>
<li><p><strong>update_centers</strong> – Whether to apply queued center update before centering</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Teacher probabilities after centering [<a href="#id14"><span class="problematic" id="id15">*</span></a>, out_dim]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.losses.dino.DINOv1Loss.update_center">
<span class="sig-name descname"><span class="pre">update_center</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">teacher_output</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/dino/#DINOv1Loss.update_center"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.dino.DINOv1Loss.update_center" title="Link to this definition">#</a></dt>
<dd><p>Queue async center update from teacher logits.</p>
<p><strong>FOR CLASSICAL CENTERING APPROACH ONLY. NOT NEEDED FOR SINKHORN-KNOPP.</strong></p>
<p>Starts an asynchronous all-reduce for distributed training. The update is
applied later when softmax_center_teacher() is called with update_centers=True.
This allows the all-reduce to overlap with backward pass for efficiency.</p>
<dl class="simple">
<dt>Typical usage:</dt><dd><p>teacher_probs = dino_loss.softmax_center_teacher(teacher_logits, temp)
loss = dino_loss(student_logits, teacher_probs)
dino_loss.update_center(teacher_logits)  # Start async update
# … backward pass happens here, overlapping with all-reduce …
# Next iteration: softmax_center_teacher() will call apply_center_update()</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>teacher_output</strong> – Teacher logits [n_views, batch_size, out_dim]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.losses.dino.DINOv2Loss">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">stable_pretraining.losses.dino.</span></span><span class="sig-name descname"><span class="pre">DINOv2Loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dino_loss_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ibot_loss_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature_student</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">center_momentum</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">student_temp</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/dino/#DINOv2Loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.dino.DINOv2Loss" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>DINOv2 loss combining CLS token and masked patch losses.</p>
<p>DINOv2 combines two losses:
- DINOv1Loss: CLS token distillation (global views) - uses Sinkhorn-Knopp
- iBOTPatchLoss: Masked patch prediction - uses Sinkhorn-Knopp</p>
<p>Both losses use Sinkhorn-Knopp normalization in DINOv2.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dino_loss_weight</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a>) – Weight for CLS token loss. Default is 1.0.</p></li>
<li><p><strong>ibot_loss_weight</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a>) – Weight for iBOT patch loss. Default is 1.0.</p></li>
<li><p><strong>temperature_student</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a>) – Temperature for student softmax in DINO. Default is 0.1.</p></li>
<li><p><strong>center_momentum</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a>) – EMA momentum for DINO centering (not used by iBOT). Default is 0.9.</p></li>
<li><p><strong>student_temp</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a>) – Temperature for student softmax in iBOT. Default is 0.1.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.losses.dino.DINOv2Loss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">student_cls_logits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">teacher_cls_probs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">student_patch_logits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">teacher_patch_probs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></span><a class="reference internal" href="../_modules/stable_pretraining/losses/dino/#DINOv2Loss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.dino.DINOv2Loss.forward" title="Link to this definition">#</a></dt>
<dd><p>Compute combined DINOv2 loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>student_cls_logits</strong> – Student CLS logits [n_views, batch, out_dim]</p></li>
<li><p><strong>teacher_cls_probs</strong> – Teacher CLS probs [n_views, batch, out_dim]</p></li>
<li><p><strong>student_patch_logits</strong> – Student patch logits [n_masked_total, patch_out_dim] or None</p></li>
<li><p><strong>teacher_patch_probs</strong> – Teacher patch probs [n_masked_total, patch_out_dim] or None</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Combined weighted loss</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="stable_pretraining.losses.dino.cross_entropy_loss">
<span class="sig-prename descclassname"><span class="pre">stable_pretraining.losses.dino.</span></span><span class="sig-name descname"><span class="pre">cross_entropy_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">s</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temp</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/dino/#cross_entropy_loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.dino.cross_entropy_loss" title="Link to this definition">#</a></dt>
<dd><p>Cross-entropy loss function for iBOT.</p>
<p>Computes per-sample cross-entropy: -Σ t[i] * log_softmax(s[i]/temp)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t</strong> – Teacher predictions (probabilities) [<a href="#id16"><span class="problematic" id="id17">*</span></a>, D]</p></li>
<li><p><strong>s</strong> – Student predictions (logits) [<a href="#id18"><span class="problematic" id="id19">*</span></a>, D]</p></li>
<li><p><strong>temp</strong> – Temperature for student softmax</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Per-sample cross-entropy loss [*] (positive, lower is better)</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.losses.dino.iBOTPatchLoss">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">stable_pretraining.losses.dino.</span></span><span class="sig-name descname"><span class="pre">iBOTPatchLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">student_temp</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/dino/#iBOTPatchLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.dino.iBOTPatchLoss" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>iBOT patch-level prediction loss for masked patch prediction.</p>
<p>This loss computes cross-entropy between teacher and student patch predictions
for masked patches only. Uses Sinkhorn-Knopp normalization exclusively (as in DINOv2/v3)
to prevent mode collapse.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>student_temp</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a>) – Temperature for student softmax. Default is 0.1.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.losses.dino.iBOTPatchLoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">student_patch_logits</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">teacher_patch_probs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/dino/#iBOTPatchLoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.dino.iBOTPatchLoss.forward" title="Link to this definition">#</a></dt>
<dd><p>Compute iBOT cross-entropy loss for masked patches.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>student_patch_logits</strong> – Student patch logits [n_masked_total, patch_out_dim]</p></li>
<li><p><strong>teacher_patch_probs</strong> – Teacher probabilities [n_masked_total, patch_out_dim]</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Scalar iBOT loss value</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.losses.dino.iBOTPatchLoss.sinkhorn_knopp_teacher">
<span class="sig-name descname"><span class="pre">sinkhorn_knopp_teacher</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">teacher_patch_tokens</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">teacher_temp</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iterations</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/dino/#iBOTPatchLoss.sinkhorn_knopp_teacher"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.dino.iBOTPatchLoss.sinkhorn_knopp_teacher" title="Link to this definition">#</a></dt>
<dd><p>Apply Sinkhorn-Knopp optimal transport normalization to teacher patch logits.</p>
<p>This method applies optimal transport to enforce exact uniform distribution across
prototypes. Used exclusively in DINOv2/v3 for iBOT patch loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>teacher_patch_tokens</strong> – Teacher patch logits [n_masked, patch_out_dim]</p></li>
<li><p><strong>teacher_temp</strong> – Temperature for softmax</p></li>
<li><p><strong>num_samples</strong> – Total number of masked patches across all GPUs (int or tensor).
If None, inferred from shape.</p></li>
<li><p><strong>n_iterations</strong> – Number of Sinkhorn iterations (default: 3)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Teacher probabilities [n_masked, patch_out_dim] with uniform prototype distribution</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-stable_pretraining.losses.joint_embedding">
<span id="stable-pretraining-losses-joint-embedding-module"></span><h2>stable_pretraining.losses.joint_embedding module<a class="headerlink" href="#module-stable_pretraining.losses.joint_embedding" title="Link to this heading">#</a></h2>
<p>Joint embedding SSL losses.</p>
<p>This module contains joint embedding methods that learn to embed different views
of the same image close together in representation space. Includes both contrastive
(NTXentLoss) and non-contrastive (BYOL, VICReg, Barlow Twins) methods.</p>
<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.losses.joint_embedding.BYOLLoss">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">stable_pretraining.losses.joint_embedding.</span></span><span class="sig-name descname"><span class="pre">BYOLLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/joint_embedding/#BYOLLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.joint_embedding.BYOLLoss" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Normalized MSE objective used in BYOL <span id="id20">[<a class="reference internal" href="../bibliography/#id3" title="Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, and others. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:21271–21284, 2020.">Grill <em>et al.</em>, 2020</a>]</span>.</p>
<p>Computes the mean squared error between L2-normalized online predictions
and L2-normalized target projections.</p>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.losses.joint_embedding.BYOLLoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">online_pred</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_proj</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></span><a class="reference internal" href="../_modules/stable_pretraining/losses/joint_embedding/#BYOLLoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.joint_embedding.BYOLLoss.forward" title="Link to this definition">#</a></dt>
<dd><p>Compute BYOL loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>online_pred</strong> – Predictions from the online network predictor.</p></li>
<li><p><strong>target_proj</strong> – Projections from the target network (no gradient).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Scalar loss value.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)">torch.Tensor</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.losses.joint_embedding.BarlowTwinsLoss">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">stable_pretraining.losses.joint_embedding.</span></span><span class="sig-name descname"><span class="pre">BarlowTwinsLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">lambd</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.005</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/joint_embedding/#BarlowTwinsLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.joint_embedding.BarlowTwinsLoss" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>SSL objective used in Barlow Twins <span id="id21">[<a class="reference internal" href="../bibliography/#id4" title="Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: self-supervised learning via redundancy reduction. In International conference on machine learning, 12310–12320. PMLR, 2021.">Zbontar <em>et al.</em>, 2021</a>]</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>lambd</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a><em>, </em><em>optional</em>) – The weight of the off-diagonal terms in the loss.
Default is 5e-3.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.losses.joint_embedding.BarlowTwinsLoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">z_i</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">z_j</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/joint_embedding/#BarlowTwinsLoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.joint_embedding.BarlowTwinsLoss.forward" title="Link to this definition">#</a></dt>
<dd><p>Compute the loss of the Barlow Twins model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>z_i</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a>) – Latent representation of the first augmented view of the batch.</p></li>
<li><p><strong>z_j</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a>) – Latent representation of the second augmented view of the batch.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The computed loss.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)">float</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.losses.joint_embedding.InfoNCELoss">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">stable_pretraining.losses.joint_embedding.</span></span><span class="sig-name descname"><span class="pre">InfoNCELoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.07</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/joint_embedding/#InfoNCELoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.joint_embedding.InfoNCELoss" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>InfoNCE contrastive loss (one-directional).</p>
<p>This module computes the cross-entropy loss between anchor embeddings
and a set of candidate embeddings, given the ground-truth targets. It
forms the core mathematical operation for losses like those in CLIP
and SimCLR.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>temperature</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a><em>, </em><em>optional</em>) – The temperature scaling factor.
Default is 0.07.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.losses.joint_embedding.InfoNCELoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">anchors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">candidates</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">targets</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logit_scale</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></span><a class="reference internal" href="../_modules/stable_pretraining/losses/joint_embedding/#InfoNCELoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.joint_embedding.InfoNCELoss.forward" title="Link to this definition">#</a></dt>
<dd><p>Computes the contrastive loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>anchors</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a>) – The primary set of embeddings (queries) of shape <cite>[N, D]</cite>.</p></li>
<li><p><strong>candidates</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a>) – The set of embeddings to contrast against (keys)
of shape <cite>[M, D]</cite>.</p></li>
<li><p><strong>targets</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a>) – A 1D tensor of ground-truth indices of shape <cite>[N]</cite>,
where <cite>targets[i]</cite> is the index of the positive candidate for <cite>anchors[i]</cite>.</p></li>
<li><p><strong>mask</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a><em>, </em><em>optional</em>) – A boolean mask of shape <cite>[N, M]</cite> to exclude
certain anchor-candidate pairs from the loss calculation. Values set to
<cite>True</cite> will be ignored.</p></li>
<li><p><strong>logit_scale</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a><em>, </em><em>optional</em>) – The temperature scaling factor.
Default is <cite>self.temperature</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A scalar loss value.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)">torch.Tensor</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.losses.joint_embedding.NTXEntLoss">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">stable_pretraining.losses.joint_embedding.</span></span><span class="sig-name descname"><span class="pre">NTXEntLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.5</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/joint_embedding/#NTXEntLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.joint_embedding.NTXEntLoss" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#stable_pretraining.losses.joint_embedding.InfoNCELoss" title="stable_pretraining.losses.joint_embedding.InfoNCELoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">InfoNCELoss</span></code></a></p>
<p>Normalized temperature-scaled cross entropy loss.</p>
<p>Introduced in the SimCLR paper <span id="id22">[<a class="reference internal" href="../bibliography/#id2" title="Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, 1597–1607. PMLR, 2020.">Chen <em>et al.</em>, 2020</a>]</span>.
Also used in MoCo <span id="id23">[<a class="reference internal" href="../bibliography/#id6" title="Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 9729–9738. 2020.">He <em>et al.</em>, 2020</a>]</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>temperature</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a><em>, </em><em>optional</em>) – The temperature scaling factor.
Default is 0.5.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.losses.joint_embedding.NTXEntLoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">z_i</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">z_j</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></span><a class="reference internal" href="../_modules/stable_pretraining/losses/joint_embedding/#NTXEntLoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.joint_embedding.NTXEntLoss.forward" title="Link to this definition">#</a></dt>
<dd><p>Compute the NT-Xent loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>z_i</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a>) – Latent representation of the first augmented view of the batch.</p></li>
<li><p><strong>z_j</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a>) – Latent representation of the second augmented view of the batch.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The computed contrastive loss.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)">float</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.losses.joint_embedding.SwAVLoss">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">stable_pretraining.losses.joint_embedding.</span></span><span class="sig-name descname"><span class="pre">SwAVLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sinkhorn_iterations</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.05</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/joint_embedding/#SwAVLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.joint_embedding.SwAVLoss" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Computes the SwAV loss, optionally using a feature queue.</p>
<p>This loss function contains the core components of the SwAV algorithm, including
the Sinkhorn-Knopp algorithm for online clustering and the swapped-prediction
contrastive task.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>temperature</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a><em>, </em><em>optional</em>) – The temperature scaling factor for the softmax
in the swapped prediction task. Default is 0.1.</p></li>
<li><p><strong>sinkhorn_iterations</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><em>int</em></a><em>, </em><em>optional</em>) – The number of iterations for the
Sinkhorn-Knopp algorithm. Default is 3.</p></li>
<li><p><strong>epsilon</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a><em>, </em><em>optional</em>) – A small value for numerical stability in the
Sinkhorn-Knopp algorithm. Default is 0.05.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Introduced in the SwAV paper <span id="id24">[<a class="reference internal" href="../bibliography/#id14" title="Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in neural information processing systems, 33:9912–9924, 2020.">Caron <em>et al.</em>, 2020</a>]</span>.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.losses.joint_embedding.SwAVLoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">proj1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">proj2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prototypes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">queue_feats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/joint_embedding/#SwAVLoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.joint_embedding.SwAVLoss.forward" title="Link to this definition">#</a></dt>
<dd><p>Compute the SwAV loss.</p>
<p>Args:
proj1 (torch.Tensor): Raw projections of the first view.
proj2 (torch.Tensor): Raw projections of the second view.
prototypes (torch.nn.Module): The prototype vectors.
queue_feats (torch.Tensor, optional): Raw features from the queue.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.losses.joint_embedding.SwAVLoss.sinkhorn">
<span class="sig-name descname"><span class="pre">sinkhorn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scores</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/joint_embedding/#SwAVLoss.sinkhorn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.joint_embedding.SwAVLoss.sinkhorn" title="Link to this definition">#</a></dt>
<dd><p>Applies the Sinkhorn-Knopp algorithm.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.losses.joint_embedding.SwAVLoss.swapped_prediction">
<span class="sig-name descname"><span class="pre">swapped_prediction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scores</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/joint_embedding/#SwAVLoss.swapped_prediction"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.joint_embedding.SwAVLoss.swapped_prediction" title="Link to this definition">#</a></dt>
<dd><p>Computes the cross-entropy loss for the swapped prediction task.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.losses.joint_embedding.VICRegLoss">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">stable_pretraining.losses.joint_embedding.</span></span><span class="sig-name descname"><span class="pre">VICRegLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sim_coeff</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">25</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">std_coeff</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">25</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cov_coeff</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0001</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/joint_embedding/#VICRegLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.joint_embedding.VICRegLoss" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>SSL objective used in VICReg <span id="id25">[<a class="reference internal" href="../bibliography/#id7" title="Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: variance-invariance-covariance regularization for self-supervised learning. arXiv preprint arXiv:2105.04906, 2021.">Bardes <em>et al.</em>, 2021</a>]</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sim_coeff</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a><em>, </em><em>optional</em>) – The weight of the similarity loss (attractive term).
Default is 25.</p></li>
<li><p><strong>std_coeff</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a><em>, </em><em>optional</em>) – The weight of the standard deviation loss.
Default is 25.</p></li>
<li><p><strong>cov_coeff</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a><em>, </em><em>optional</em>) – The weight of the covariance loss.
Default is 1.</p></li>
<li><p><strong>epsilon</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a><em>, </em><em>optional</em>) – Small value to avoid division by zero.
Default is 1e-4.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.losses.joint_embedding.VICRegLoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">z_i</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">z_j</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/joint_embedding/#VICRegLoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.joint_embedding.VICRegLoss.forward" title="Link to this definition">#</a></dt>
<dd><p>Compute the loss of the VICReg model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>z_i</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a>) – Latent representation of the first augmented view of the batch.</p></li>
<li><p><strong>z_j</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a>) – Latent representation of the second augmented view of the batch.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The computed loss.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)">float</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-stable_pretraining.losses.multimodal">
<span id="stable-pretraining-losses-multimodal-module"></span><h2>stable_pretraining.losses.multimodal module<a class="headerlink" href="#module-stable_pretraining.losses.multimodal" title="Link to this heading">#</a></h2>
<p>Multimodal SSL losses.</p>
<p>This module contains losses for multimodal self-supervised learning,
particularly for image-text contrastive learning like CLIP.</p>
<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.losses.multimodal.CLIPLoss">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">stable_pretraining.losses.multimodal.</span></span><span class="sig-name descname"><span class="pre">CLIPLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.07</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/multimodal/#CLIPLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.multimodal.CLIPLoss" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#stable_pretraining.losses.joint_embedding.InfoNCELoss" title="stable_pretraining.losses.joint_embedding.InfoNCELoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">InfoNCELoss</span></code></a></p>
<p>CLIP loss (symmetric bidirectional InfoNCE).</p>
<p>As used in CLIP <span id="id26">[<a class="reference internal" href="../bibliography/#id13" title="Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, and others. Learning transferable visual models from natural language supervision. In International conference on machine learning, 8748–8763. PmLR, 2021.">Radford <em>et al.</em>, 2021</a>]</span>.
Computes symmetric cross-entropy over image-text and text-image logits.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>temperature</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a><em>, </em><em>optional</em>) – Softmax temperature. Default is 0.07.
(If you use a learnable logit_scale in your model, pass it to
forward(…) and this temperature will be ignored.)</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.losses.multimodal.CLIPLoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">feats_i</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">feats_j</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">logit_scale</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></span><a class="reference internal" href="../_modules/stable_pretraining/losses/multimodal/#CLIPLoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.multimodal.CLIPLoss.forward" title="Link to this definition">#</a></dt>
<dd><p>Computes the contrastive loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>anchors</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a>) – The primary set of embeddings (queries) of shape <cite>[N, D]</cite>.</p></li>
<li><p><strong>candidates</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a>) – The set of embeddings to contrast against (keys)
of shape <cite>[M, D]</cite>.</p></li>
<li><p><strong>targets</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a>) – A 1D tensor of ground-truth indices of shape <cite>[N]</cite>,
where <cite>targets[i]</cite> is the index of the positive candidate for <cite>anchors[i]</cite>.</p></li>
<li><p><strong>mask</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a><em>, </em><em>optional</em>) – A boolean mask of shape <cite>[N, M]</cite> to exclude
certain anchor-candidate pairs from the loss calculation. Values set to
<cite>True</cite> will be ignored.</p></li>
<li><p><strong>logit_scale</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a><em>, </em><em>optional</em>) – The temperature scaling factor.
Default is <cite>self.temperature</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A scalar loss value.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)">torch.Tensor</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-stable_pretraining.losses.reconstruction">
<span id="stable-pretraining-losses-reconstruction-module"></span><h2>stable_pretraining.losses.reconstruction module<a class="headerlink" href="#module-stable_pretraining.losses.reconstruction" title="Link to this heading">#</a></h2>
<p>Reconstruction-based SSL losses.</p>
<p>This module contains reconstruction-based self-supervised learning losses
such as Masked Autoencoder (MAE).</p>
<dl class="py function">
<dt class="sig sig-object py" id="stable_pretraining.losses.reconstruction.mae">
<span class="sig-prename descclassname"><span class="pre">stable_pretraining.losses.reconstruction.</span></span><span class="sig-name descname"><span class="pre">mae</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_pix_loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/reconstruction/#mae"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.reconstruction.mae" title="Link to this definition">#</a></dt>
<dd><p>Compute masked autoencoder loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>target</strong> – [N, L, p*p*3] target images</p></li>
<li><p><strong>pred</strong> – [N, L, p*p*3] predicted images</p></li>
<li><p><strong>mask</strong> – [N, L], 0 is keep, 1 is remove</p></li>
<li><p><strong>norm_pix_loss</strong> – whether to normalize pixels</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>mean loss value</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>loss</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-stable_pretraining.losses.utils">
<span id="stable-pretraining-losses-utils-module"></span><h2>stable_pretraining.losses.utils module<a class="headerlink" href="#module-stable_pretraining.losses.utils" title="Link to this heading">#</a></h2>
<p>Utilities for SSL losses.</p>
<p>This module provides helper functions and utilities used by various SSL losses,
such as Sinkhorn-Knopp optimal transport algorithm for DINO and iBOT.</p>
<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.losses.utils.NegativeCosineSimilarity">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">stable_pretraining.losses.utils.</span></span><span class="sig-name descname"><span class="pre">NegativeCosineSimilarity</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/utils/#NegativeCosineSimilarity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.utils.NegativeCosineSimilarity" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Negative cosine similarity objective.</p>
<p>This objective is used for instance in BYOL <span id="id27">[<a class="reference internal" href="../bibliography/#id3" title="Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, and others. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:21271–21284, 2020.">Grill <em>et al.</em>, 2020</a>]</span>
or SimSiam <span id="id28">[<a class="reference internal" href="../bibliography/#id5" title="Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 15750–15758. 2021.">Chen and He, 2021</a>]</span>.</p>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.losses.utils.NegativeCosineSimilarity.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">z_i</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">z_j</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/utils/#NegativeCosineSimilarity.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.utils.NegativeCosineSimilarity.forward" title="Link to this definition">#</a></dt>
<dd><p>Compute the loss of the BYOL model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>z_i</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a>) – Latent representation of the first augmented view of the batch.</p></li>
<li><p><strong>z_j</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a>) – Latent representation of the second augmented view of the batch.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The computed loss.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)">float</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="stable_pretraining.losses.utils.off_diagonal">
<span class="sig-prename descclassname"><span class="pre">stable_pretraining.losses.utils.</span></span><span class="sig-name descname"><span class="pre">off_diagonal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/utils/#off_diagonal"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.utils.off_diagonal" title="Link to this definition">#</a></dt>
<dd><p>Return a flattened view of the off-diagonal elements of a square matrix.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="stable_pretraining.losses.utils.sinkhorn_knopp">
<span class="sig-prename descclassname"><span class="pre">stable_pretraining.losses.utils.</span></span><span class="sig-name descname"><span class="pre">sinkhorn_knopp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">teacher_output</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">teacher_temp</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_samples</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iterations</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">3</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></span><a class="reference internal" href="../_modules/stable_pretraining/losses/utils/#sinkhorn_knopp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.utils.sinkhorn_knopp" title="Link to this definition">#</a></dt>
<dd><p>Sinkhorn-Knopp algorithm for optimal transport normalization.</p>
<p>This is an alternative to simple centering used in DINO/iBOT losses.
It performs optimal transport to assign samples to prototypes, ensuring
a more uniform distribution across prototypes.</p>
<p>Reference: DINOv3 implementation
<a class="github reference external" href="https://github.com/facebookresearch/dinov3">facebookresearch/dinov3</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>teacher_output</strong> – Teacher predictions [batch, prototypes] or [n_samples, prototypes]</p></li>
<li><p><strong>teacher_temp</strong> – Temperature for softmax</p></li>
<li><p><strong>num_samples</strong> – Number of samples to assign. Can be:
- int: Fixed batch size (e.g., batch_size * world_size for DINO)
- torch.Tensor: Variable count (e.g., n_masked_patches for iBOT)</p></li>
<li><p><strong>n_iterations</strong> – Number of Sinkhorn iterations (default: 3)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Normalized probabilities [batch, prototypes] summing to 1 over prototypes</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<p># DINO CLS token loss (fixed batch size)
Q = sinkhorn_knopp(teacher_cls_output, temp=0.04,</p>
<blockquote>
<div><p>num_samples=batch_size * world_size)</p>
</div></blockquote>
<p># iBOT patch loss (variable number of masked patches)
Q = sinkhorn_knopp(teacher_patch_output, temp=0.04,</p>
<blockquote>
<div><p>num_samples=n_masked_patches_tensor)</p>
</div></blockquote>
</dd></dl>

</section>
<section id="module-stable_pretraining.losses">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-stable_pretraining.losses" title="Link to this heading">#</a></h2>
<p>SSL Losses.</p>
<p>This module provides various self-supervised learning loss functions organized by category:
- DINO losses: Self-distillation methods (DINOLoss, iBOTPatchLoss)
- Joint embedding losses: Contrastive and non-contrastive methods (BYOL, VICReg, Barlow Twins, SimCLR)
- Reconstruction losses: Masked prediction methods (MAE)
- Utilities: Helper functions (sinkhorn_knopp, off_diagonal, NegativeCosineSimilarity)</p>
<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.losses.BYOLLoss">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">stable_pretraining.losses.</span></span><span class="sig-name descname"><span class="pre">BYOLLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/joint_embedding/#BYOLLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.BYOLLoss" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Normalized MSE objective used in BYOL <span id="id29">[<a class="reference internal" href="../bibliography/#id3" title="Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, and others. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:21271–21284, 2020.">Grill <em>et al.</em>, 2020</a>]</span>.</p>
<p>Computes the mean squared error between L2-normalized online predictions
and L2-normalized target projections.</p>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.losses.BYOLLoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">online_pred</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_proj</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></span><a class="reference internal" href="../_modules/stable_pretraining/losses/joint_embedding/#BYOLLoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.BYOLLoss.forward" title="Link to this definition">#</a></dt>
<dd><p>Compute BYOL loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>online_pred</strong> – Predictions from the online network predictor.</p></li>
<li><p><strong>target_proj</strong> – Projections from the target network (no gradient).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Scalar loss value.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)">torch.Tensor</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.losses.BarlowTwinsLoss">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">stable_pretraining.losses.</span></span><span class="sig-name descname"><span class="pre">BarlowTwinsLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">lambd</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.005</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/joint_embedding/#BarlowTwinsLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.BarlowTwinsLoss" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>SSL objective used in Barlow Twins <span id="id30">[<a class="reference internal" href="../bibliography/#id4" title="Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: self-supervised learning via redundancy reduction. In International conference on machine learning, 12310–12320. PMLR, 2021.">Zbontar <em>et al.</em>, 2021</a>]</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>lambd</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a><em>, </em><em>optional</em>) – The weight of the off-diagonal terms in the loss.
Default is 5e-3.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.losses.BarlowTwinsLoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">z_i</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">z_j</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/joint_embedding/#BarlowTwinsLoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.BarlowTwinsLoss.forward" title="Link to this definition">#</a></dt>
<dd><p>Compute the loss of the Barlow Twins model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>z_i</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a>) – Latent representation of the first augmented view of the batch.</p></li>
<li><p><strong>z_j</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a>) – Latent representation of the second augmented view of the batch.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The computed loss.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)">float</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.losses.CLIPLoss">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">stable_pretraining.losses.</span></span><span class="sig-name descname"><span class="pre">CLIPLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.07</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/multimodal/#CLIPLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.CLIPLoss" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#stable_pretraining.losses.joint_embedding.InfoNCELoss" title="stable_pretraining.losses.joint_embedding.InfoNCELoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">InfoNCELoss</span></code></a></p>
<p>CLIP loss (symmetric bidirectional InfoNCE).</p>
<p>As used in CLIP <span id="id31">[<a class="reference internal" href="../bibliography/#id13" title="Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, and others. Learning transferable visual models from natural language supervision. In International conference on machine learning, 8748–8763. PmLR, 2021.">Radford <em>et al.</em>, 2021</a>]</span>.
Computes symmetric cross-entropy over image-text and text-image logits.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>temperature</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a><em>, </em><em>optional</em>) – Softmax temperature. Default is 0.07.
(If you use a learnable logit_scale in your model, pass it to
forward(…) and this temperature will be ignored.)</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.losses.CLIPLoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">feats_i</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">feats_j</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">logit_scale</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></span><a class="reference internal" href="../_modules/stable_pretraining/losses/multimodal/#CLIPLoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.CLIPLoss.forward" title="Link to this definition">#</a></dt>
<dd><p>Computes the contrastive loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>anchors</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a>) – The primary set of embeddings (queries) of shape <cite>[N, D]</cite>.</p></li>
<li><p><strong>candidates</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a>) – The set of embeddings to contrast against (keys)
of shape <cite>[M, D]</cite>.</p></li>
<li><p><strong>targets</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a>) – A 1D tensor of ground-truth indices of shape <cite>[N]</cite>,
where <cite>targets[i]</cite> is the index of the positive candidate for <cite>anchors[i]</cite>.</p></li>
<li><p><strong>mask</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a><em>, </em><em>optional</em>) – A boolean mask of shape <cite>[N, M]</cite> to exclude
certain anchor-candidate pairs from the loss calculation. Values set to
<cite>True</cite> will be ignored.</p></li>
<li><p><strong>logit_scale</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a><em>, </em><em>optional</em>) – The temperature scaling factor.
Default is <cite>self.temperature</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A scalar loss value.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)">torch.Tensor</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.losses.DINOv1Loss">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">stable_pretraining.losses.</span></span><span class="sig-name descname"><span class="pre">DINOv1Loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">temperature_student</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">center_momentum</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.9</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/dino/#DINOv1Loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.DINOv1Loss" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>DINOv1 loss for self-distillation with cross-entropy <span id="id32">[<a class="reference internal" href="../bibliography/#id9" title="Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, 9650–9660. 2021.">Caron <em>et al.</em>, 2021</a>]</span>.</p>
<p>This loss computes cross-entropy between teacher and student logits after applying
temperature scaling and normalization. The teacher uses either classical centering or
Sinkhorn-Knopp normalization to prevent mode collapse.</p>
<dl>
<dt>Usage:</dt><dd><p><a href="#id33"><span class="problematic" id="id34">``</span></a><a href="#id35"><span class="problematic" id="id36">`</span></a>python
dino_loss = DINOv1Loss()</p>
<p># Get logits from prototype layer
student_logits = prototype_layer(student_embeddings)  # [n_views, B, out_dim]
teacher_logits = prototype_layer(teacher_embeddings)  # [n_views, B, out_dim]</p>
<p># Approach 1: Classical centering (recommended, faster)
teacher_probs = dino_loss.softmax_center_teacher(teacher_logits, temp=0.04)
loss = dino_loss(student_logits, teacher_probs)
dino_loss.update_center(teacher_logits)  # Queue async center update</p>
<p># Approach 2: Sinkhorn-Knopp (more principled, slower, no centering needed)
n_views, batch_size, _ = teacher_logits.shape
num_samples = n_views * batch_size  # Total samples across views
teacher_probs = dino_loss.sinkhorn_knopp_teacher(</p>
<blockquote>
<div><p>teacher_logits, temp=0.04, num_samples=num_samples</p>
</div></blockquote>
<p>)
loss = dino_loss(student_logits, teacher_probs)
# No update_center() needed for Sinkhorn-Knopp!
<a href="#id37"><span class="problematic" id="id38">``</span></a><a href="#id39"><span class="problematic" id="id40">`</span></a></p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>temperature_student</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a>) – Temperature for student softmax. Default is 0.1.</p></li>
<li><p><strong>center_momentum</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a>) – EMA momentum for center update. Default is 0.9.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.losses.DINOv1Loss.apply_center_update">
<span class="sig-name descname"><span class="pre">apply_center_update</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/dino/#DINOv1Loss.apply_center_update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.DINOv1Loss.apply_center_update" title="Link to this definition">#</a></dt>
<dd><p>Apply the queued center update with EMA.</p>
<p><strong>FOR CLASSICAL CENTERING APPROACH ONLY. NOT NEEDED FOR SINKHORN-KNOPP.</strong></p>
<p>Waits for async all-reduce to complete and updates self.center with EMA.
Automatically called by softmax_center_teacher() if update_centers=True.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.losses.DINOv1Loss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">student_logits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">teacher_probs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></span><a class="reference internal" href="../_modules/stable_pretraining/losses/dino/#DINOv1Loss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.DINOv1Loss.forward" title="Link to this definition">#</a></dt>
<dd><p>Compute DINO cross-entropy loss.</p>
<p>This is a pure loss computation with no side effects (no centering, no updates).
Teacher probabilities should be pre-processed with softmax_center_teacher() or
sinkhorn_knopp_teacher(). Center updates should be done separately with update_center().</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>student_logits</strong> – Student logits [n_views, batch_size, out_dim]</p></li>
<li><p><strong>teacher_probs</strong> – Teacher probabilities (already normalized) [n_views, batch_size, out_dim]</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Scalar DINO loss value (cross-entropy averaged over view pairs, excluding diagonal)</p>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>student_logits: (S, B, K) where S = student views, B = batch size, K = out_dim</p></li>
<li><p>teacher_probs: (T, B, K) where T = teacher views</p></li>
<li><p>output: scalar</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.losses.DINOv1Loss.sinkhorn_knopp_teacher">
<span class="sig-name descname"><span class="pre">sinkhorn_knopp_teacher</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">teacher_logits</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">teacher_temp</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iterations</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/dino/#DINOv1Loss.sinkhorn_knopp_teacher"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.DINOv1Loss.sinkhorn_knopp_teacher" title="Link to this definition">#</a></dt>
<dd><p>Apply Sinkhorn-Knopp optimal transport normalization to teacher logits.</p>
<p><strong>FOR SINKHORN-KNOPP APPROACH ONLY. DOES NOT USE CENTER.</strong></p>
<p>This method applies sinkhorn-knopp to enforce exact uniform distribution across
prototypes without using centering. More principled than centering but more expensive.
Used in SwAV and DINOv3 for better theoretical guarantees.</p>
<p>Note: When using Sinkhorn-Knopp, you do NOT need to call update_center() or
apply_center_update() since centering is not used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>teacher_logits</strong> – Teacher logits [<a href="#id41"><span class="problematic" id="id42">*</span></a>, out_dim]. Can be any shape as long as last dim is out_dim.
Common shapes: [batch, out_dim] or [n_views, batch, out_dim]</p></li>
<li><p><strong>teacher_temp</strong> – Temperature for softmax</p></li>
<li><p><strong>num_samples</strong> – Total number of samples across all GPUs (int or tensor).
If None, inferred from shape assuming [batch, out_dim] format.
For multi-view [n_views, batch, out_dim], pass n_views * batch explicitly.</p></li>
<li><p><strong>n_iterations</strong> – Number of Sinkhorn iterations (default: 3)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Teacher probabilities [same shape as input] with uniform prototype distribution</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.losses.DINOv1Loss.softmax_center_teacher">
<span class="sig-name descname"><span class="pre">softmax_center_teacher</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">teacher_logits</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">teacher_temp</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">update_centers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/dino/#DINOv1Loss.softmax_center_teacher"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.DINOv1Loss.softmax_center_teacher" title="Link to this definition">#</a></dt>
<dd><p>Apply classical centering and sharpening to teacher logits.</p>
<p><strong>FOR CLASSICAL CENTERING APPROACH ONLY. NOT NEEDED FOR SINKHORN-KNOPP.</strong></p>
<p>This method subtracts the center (EMA of batch means) from teacher logits before
applying softmax. This prevents mode collapse by ensuring balanced prototype usage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>teacher_logits</strong> – Teacher logits [<a href="#id43"><span class="problematic" id="id44">*</span></a>, out_dim]</p></li>
<li><p><strong>teacher_temp</strong> – Temperature for teacher softmax</p></li>
<li><p><strong>update_centers</strong> – Whether to apply queued center update before centering</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Teacher probabilities after centering [<a href="#id45"><span class="problematic" id="id46">*</span></a>, out_dim]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.losses.DINOv1Loss.update_center">
<span class="sig-name descname"><span class="pre">update_center</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">teacher_output</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/dino/#DINOv1Loss.update_center"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.DINOv1Loss.update_center" title="Link to this definition">#</a></dt>
<dd><p>Queue async center update from teacher logits.</p>
<p><strong>FOR CLASSICAL CENTERING APPROACH ONLY. NOT NEEDED FOR SINKHORN-KNOPP.</strong></p>
<p>Starts an asynchronous all-reduce for distributed training. The update is
applied later when softmax_center_teacher() is called with update_centers=True.
This allows the all-reduce to overlap with backward pass for efficiency.</p>
<dl class="simple">
<dt>Typical usage:</dt><dd><p>teacher_probs = dino_loss.softmax_center_teacher(teacher_logits, temp)
loss = dino_loss(student_logits, teacher_probs)
dino_loss.update_center(teacher_logits)  # Start async update
# … backward pass happens here, overlapping with all-reduce …
# Next iteration: softmax_center_teacher() will call apply_center_update()</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>teacher_output</strong> – Teacher logits [n_views, batch_size, out_dim]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.losses.DINOv2Loss">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">stable_pretraining.losses.</span></span><span class="sig-name descname"><span class="pre">DINOv2Loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dino_loss_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ibot_loss_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature_student</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">center_momentum</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">student_temp</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/dino/#DINOv2Loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.DINOv2Loss" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>DINOv2 loss combining CLS token and masked patch losses.</p>
<p>DINOv2 combines two losses:
- DINOv1Loss: CLS token distillation (global views) - uses Sinkhorn-Knopp
- iBOTPatchLoss: Masked patch prediction - uses Sinkhorn-Knopp</p>
<p>Both losses use Sinkhorn-Knopp normalization in DINOv2.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dino_loss_weight</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a>) – Weight for CLS token loss. Default is 1.0.</p></li>
<li><p><strong>ibot_loss_weight</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a>) – Weight for iBOT patch loss. Default is 1.0.</p></li>
<li><p><strong>temperature_student</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a>) – Temperature for student softmax in DINO. Default is 0.1.</p></li>
<li><p><strong>center_momentum</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a>) – EMA momentum for DINO centering (not used by iBOT). Default is 0.9.</p></li>
<li><p><strong>student_temp</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a>) – Temperature for student softmax in iBOT. Default is 0.1.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.losses.DINOv2Loss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">student_cls_logits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">teacher_cls_probs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">student_patch_logits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">teacher_patch_probs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></span><a class="reference internal" href="../_modules/stable_pretraining/losses/dino/#DINOv2Loss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.DINOv2Loss.forward" title="Link to this definition">#</a></dt>
<dd><p>Compute combined DINOv2 loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>student_cls_logits</strong> – Student CLS logits [n_views, batch, out_dim]</p></li>
<li><p><strong>teacher_cls_probs</strong> – Teacher CLS probs [n_views, batch, out_dim]</p></li>
<li><p><strong>student_patch_logits</strong> – Student patch logits [n_masked_total, patch_out_dim] or None</p></li>
<li><p><strong>teacher_patch_probs</strong> – Teacher patch probs [n_masked_total, patch_out_dim] or None</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Combined weighted loss</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.losses.NTXEntLoss">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">stable_pretraining.losses.</span></span><span class="sig-name descname"><span class="pre">NTXEntLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.5</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/joint_embedding/#NTXEntLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.NTXEntLoss" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#stable_pretraining.losses.joint_embedding.InfoNCELoss" title="stable_pretraining.losses.joint_embedding.InfoNCELoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">InfoNCELoss</span></code></a></p>
<p>Normalized temperature-scaled cross entropy loss.</p>
<p>Introduced in the SimCLR paper <span id="id47">[<a class="reference internal" href="../bibliography/#id2" title="Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, 1597–1607. PMLR, 2020.">Chen <em>et al.</em>, 2020</a>]</span>.
Also used in MoCo <span id="id48">[<a class="reference internal" href="../bibliography/#id6" title="Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 9729–9738. 2020.">He <em>et al.</em>, 2020</a>]</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>temperature</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a><em>, </em><em>optional</em>) – The temperature scaling factor.
Default is 0.5.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.losses.NTXEntLoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">z_i</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">z_j</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></span><a class="reference internal" href="../_modules/stable_pretraining/losses/joint_embedding/#NTXEntLoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.NTXEntLoss.forward" title="Link to this definition">#</a></dt>
<dd><p>Compute the NT-Xent loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>z_i</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a>) – Latent representation of the first augmented view of the batch.</p></li>
<li><p><strong>z_j</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a>) – Latent representation of the second augmented view of the batch.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The computed contrastive loss.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)">float</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.losses.NegativeCosineSimilarity">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">stable_pretraining.losses.</span></span><span class="sig-name descname"><span class="pre">NegativeCosineSimilarity</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/utils/#NegativeCosineSimilarity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.NegativeCosineSimilarity" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Negative cosine similarity objective.</p>
<p>This objective is used for instance in BYOL <span id="id49">[<a class="reference internal" href="../bibliography/#id3" title="Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, and others. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:21271–21284, 2020.">Grill <em>et al.</em>, 2020</a>]</span>
or SimSiam <span id="id50">[<a class="reference internal" href="../bibliography/#id5" title="Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 15750–15758. 2021.">Chen and He, 2021</a>]</span>.</p>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.losses.NegativeCosineSimilarity.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">z_i</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">z_j</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/utils/#NegativeCosineSimilarity.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.NegativeCosineSimilarity.forward" title="Link to this definition">#</a></dt>
<dd><p>Compute the loss of the BYOL model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>z_i</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a>) – Latent representation of the first augmented view of the batch.</p></li>
<li><p><strong>z_j</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a>) – Latent representation of the second augmented view of the batch.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The computed loss.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)">float</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.losses.VICRegLoss">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">stable_pretraining.losses.</span></span><span class="sig-name descname"><span class="pre">VICRegLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sim_coeff</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">25</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">std_coeff</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">25</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cov_coeff</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0001</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/joint_embedding/#VICRegLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.VICRegLoss" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>SSL objective used in VICReg <span id="id51">[<a class="reference internal" href="../bibliography/#id7" title="Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: variance-invariance-covariance regularization for self-supervised learning. arXiv preprint arXiv:2105.04906, 2021.">Bardes <em>et al.</em>, 2021</a>]</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sim_coeff</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a><em>, </em><em>optional</em>) – The weight of the similarity loss (attractive term).
Default is 25.</p></li>
<li><p><strong>std_coeff</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a><em>, </em><em>optional</em>) – The weight of the standard deviation loss.
Default is 25.</p></li>
<li><p><strong>cov_coeff</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a><em>, </em><em>optional</em>) – The weight of the covariance loss.
Default is 1.</p></li>
<li><p><strong>epsilon</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a><em>, </em><em>optional</em>) – Small value to avoid division by zero.
Default is 1e-4.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.losses.VICRegLoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">z_i</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">z_j</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/joint_embedding/#VICRegLoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.VICRegLoss.forward" title="Link to this definition">#</a></dt>
<dd><p>Compute the loss of the VICReg model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>z_i</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a>) – Latent representation of the first augmented view of the batch.</p></li>
<li><p><strong>z_j</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a>) – Latent representation of the second augmented view of the batch.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The computed loss.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)">float</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.losses.iBOTPatchLoss">
<span class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></span><span class="sig-prename descclassname"><span class="pre">stable_pretraining.losses.</span></span><span class="sig-name descname"><span class="pre">iBOTPatchLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">student_temp</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/dino/#iBOTPatchLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.iBOTPatchLoss" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>iBOT patch-level prediction loss for masked patch prediction.</p>
<p>This loss computes cross-entropy between teacher and student patch predictions
for masked patches only. Uses Sinkhorn-Knopp normalization exclusively (as in DINOv2/v3)
to prevent mode collapse.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>student_temp</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a>) – Temperature for student softmax. Default is 0.1.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.losses.iBOTPatchLoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">student_patch_logits</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">teacher_patch_probs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/dino/#iBOTPatchLoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.iBOTPatchLoss.forward" title="Link to this definition">#</a></dt>
<dd><p>Compute iBOT cross-entropy loss for masked patches.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>student_patch_logits</strong> – Student patch logits [n_masked_total, patch_out_dim]</p></li>
<li><p><strong>teacher_patch_probs</strong> – Teacher probabilities [n_masked_total, patch_out_dim]</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Scalar iBOT loss value</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.losses.iBOTPatchLoss.sinkhorn_knopp_teacher">
<span class="sig-name descname"><span class="pre">sinkhorn_knopp_teacher</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">teacher_patch_tokens</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">teacher_temp</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iterations</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/dino/#iBOTPatchLoss.sinkhorn_knopp_teacher"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.iBOTPatchLoss.sinkhorn_knopp_teacher" title="Link to this definition">#</a></dt>
<dd><p>Apply Sinkhorn-Knopp optimal transport normalization to teacher patch logits.</p>
<p>This method applies optimal transport to enforce exact uniform distribution across
prototypes. Used exclusively in DINOv2/v3 for iBOT patch loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>teacher_patch_tokens</strong> – Teacher patch logits [n_masked, patch_out_dim]</p></li>
<li><p><strong>teacher_temp</strong> – Temperature for softmax</p></li>
<li><p><strong>num_samples</strong> – Total number of masked patches across all GPUs (int or tensor).
If None, inferred from shape.</p></li>
<li><p><strong>n_iterations</strong> – Number of Sinkhorn iterations (default: 3)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Teacher probabilities [n_masked, patch_out_dim] with uniform prototype distribution</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="stable_pretraining.losses.mae">
<span class="sig-prename descclassname"><span class="pre">stable_pretraining.losses.</span></span><span class="sig-name descname"><span class="pre">mae</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_pix_loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/reconstruction/#mae"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.mae" title="Link to this definition">#</a></dt>
<dd><p>Compute masked autoencoder loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>target</strong> – [N, L, p*p*3] target images</p></li>
<li><p><strong>pred</strong> – [N, L, p*p*3] predicted images</p></li>
<li><p><strong>mask</strong> – [N, L], 0 is keep, 1 is remove</p></li>
<li><p><strong>norm_pix_loss</strong> – whether to normalize pixels</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>mean loss value</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>loss</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="stable_pretraining.losses.off_diagonal">
<span class="sig-prename descclassname"><span class="pre">stable_pretraining.losses.</span></span><span class="sig-name descname"><span class="pre">off_diagonal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/losses/utils/#off_diagonal"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.off_diagonal" title="Link to this definition">#</a></dt>
<dd><p>Return a flattened view of the off-diagonal elements of a square matrix.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="stable_pretraining.losses.sinkhorn_knopp">
<span class="sig-prename descclassname"><span class="pre">stable_pretraining.losses.</span></span><span class="sig-name descname"><span class="pre">sinkhorn_knopp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">teacher_output</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">teacher_temp</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_samples</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iterations</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">3</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></span><a class="reference internal" href="../_modules/stable_pretraining/losses/utils/#sinkhorn_knopp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.losses.sinkhorn_knopp" title="Link to this definition">#</a></dt>
<dd><p>Sinkhorn-Knopp algorithm for optimal transport normalization.</p>
<p>This is an alternative to simple centering used in DINO/iBOT losses.
It performs optimal transport to assign samples to prototypes, ensuring
a more uniform distribution across prototypes.</p>
<p>Reference: DINOv3 implementation
<a class="github reference external" href="https://github.com/facebookresearch/dinov3">facebookresearch/dinov3</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>teacher_output</strong> – Teacher predictions [batch, prototypes] or [n_samples, prototypes]</p></li>
<li><p><strong>teacher_temp</strong> – Temperature for softmax</p></li>
<li><p><strong>num_samples</strong> – Number of samples to assign. Can be:
- int: Fixed batch size (e.g., batch_size * world_size for DINO)
- torch.Tensor: Variable count (e.g., n_masked_patches for iBOT)</p></li>
<li><p><strong>n_iterations</strong> – Number of Sinkhorn iterations (default: 3)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Normalized probabilities [batch, prototypes] summing to 1 over prototypes</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<p># DINO CLS token loss (fixed batch size)
Q = sinkhorn_knopp(teacher_cls_output, temp=0.04,</p>
<blockquote>
<div><p>num_samples=batch_size * world_size)</p>
</div></blockquote>
<p># iBOT patch loss (variable number of masked patches)
Q = sinkhorn_knopp(teacher_patch_output, temp=0.04,</p>
<blockquote>
<div><p>num_samples=n_masked_patches_tensor)</p>
</div></blockquote>
</dd></dl>

</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#submodules">Submodules</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-stable_pretraining.losses.dino">stable_pretraining.losses.dino module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.dino.DINOv1Loss"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.dino.DINOv1Loss.apply_center_update"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss.apply_center_update()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.dino.DINOv1Loss.forward"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.dino.DINOv1Loss.sinkhorn_knopp_teacher"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss.sinkhorn_knopp_teacher()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.dino.DINOv1Loss.softmax_center_teacher"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss.softmax_center_teacher()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.dino.DINOv1Loss.update_center"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss.update_center()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.dino.DINOv2Loss"><code class="docutils literal notranslate"><span class="pre">DINOv2Loss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.dino.DINOv2Loss.forward"><code class="docutils literal notranslate"><span class="pre">DINOv2Loss.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.dino.cross_entropy_loss"><code class="docutils literal notranslate"><span class="pre">cross_entropy_loss()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.dino.iBOTPatchLoss"><code class="docutils literal notranslate"><span class="pre">iBOTPatchLoss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.dino.iBOTPatchLoss.forward"><code class="docutils literal notranslate"><span class="pre">iBOTPatchLoss.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.dino.iBOTPatchLoss.sinkhorn_knopp_teacher"><code class="docutils literal notranslate"><span class="pre">iBOTPatchLoss.sinkhorn_knopp_teacher()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-stable_pretraining.losses.joint_embedding">stable_pretraining.losses.joint_embedding module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.joint_embedding.BYOLLoss"><code class="docutils literal notranslate"><span class="pre">BYOLLoss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.joint_embedding.BYOLLoss.forward"><code class="docutils literal notranslate"><span class="pre">BYOLLoss.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.joint_embedding.BarlowTwinsLoss"><code class="docutils literal notranslate"><span class="pre">BarlowTwinsLoss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.joint_embedding.BarlowTwinsLoss.forward"><code class="docutils literal notranslate"><span class="pre">BarlowTwinsLoss.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.joint_embedding.InfoNCELoss"><code class="docutils literal notranslate"><span class="pre">InfoNCELoss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.joint_embedding.InfoNCELoss.forward"><code class="docutils literal notranslate"><span class="pre">InfoNCELoss.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.joint_embedding.NTXEntLoss"><code class="docutils literal notranslate"><span class="pre">NTXEntLoss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.joint_embedding.NTXEntLoss.forward"><code class="docutils literal notranslate"><span class="pre">NTXEntLoss.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.joint_embedding.SwAVLoss"><code class="docutils literal notranslate"><span class="pre">SwAVLoss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.joint_embedding.SwAVLoss.forward"><code class="docutils literal notranslate"><span class="pre">SwAVLoss.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.joint_embedding.SwAVLoss.sinkhorn"><code class="docutils literal notranslate"><span class="pre">SwAVLoss.sinkhorn()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.joint_embedding.SwAVLoss.swapped_prediction"><code class="docutils literal notranslate"><span class="pre">SwAVLoss.swapped_prediction()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.joint_embedding.VICRegLoss"><code class="docutils literal notranslate"><span class="pre">VICRegLoss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.joint_embedding.VICRegLoss.forward"><code class="docutils literal notranslate"><span class="pre">VICRegLoss.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-stable_pretraining.losses.multimodal">stable_pretraining.losses.multimodal module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.multimodal.CLIPLoss"><code class="docutils literal notranslate"><span class="pre">CLIPLoss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.multimodal.CLIPLoss.forward"><code class="docutils literal notranslate"><span class="pre">CLIPLoss.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-stable_pretraining.losses.reconstruction">stable_pretraining.losses.reconstruction module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.reconstruction.mae"><code class="docutils literal notranslate"><span class="pre">mae()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-stable_pretraining.losses.utils">stable_pretraining.losses.utils module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.utils.NegativeCosineSimilarity"><code class="docutils literal notranslate"><span class="pre">NegativeCosineSimilarity</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.utils.NegativeCosineSimilarity.forward"><code class="docutils literal notranslate"><span class="pre">NegativeCosineSimilarity.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.utils.off_diagonal"><code class="docutils literal notranslate"><span class="pre">off_diagonal()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.utils.sinkhorn_knopp"><code class="docutils literal notranslate"><span class="pre">sinkhorn_knopp()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-stable_pretraining.losses">Module contents</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.BYOLLoss"><code class="docutils literal notranslate"><span class="pre">BYOLLoss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.BYOLLoss.forward"><code class="docutils literal notranslate"><span class="pre">BYOLLoss.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.BarlowTwinsLoss"><code class="docutils literal notranslate"><span class="pre">BarlowTwinsLoss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.BarlowTwinsLoss.forward"><code class="docutils literal notranslate"><span class="pre">BarlowTwinsLoss.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.CLIPLoss"><code class="docutils literal notranslate"><span class="pre">CLIPLoss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.CLIPLoss.forward"><code class="docutils literal notranslate"><span class="pre">CLIPLoss.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.DINOv1Loss"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.DINOv1Loss.apply_center_update"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss.apply_center_update()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.DINOv1Loss.forward"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.DINOv1Loss.sinkhorn_knopp_teacher"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss.sinkhorn_knopp_teacher()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.DINOv1Loss.softmax_center_teacher"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss.softmax_center_teacher()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.DINOv1Loss.update_center"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss.update_center()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.DINOv2Loss"><code class="docutils literal notranslate"><span class="pre">DINOv2Loss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.DINOv2Loss.forward"><code class="docutils literal notranslate"><span class="pre">DINOv2Loss.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.NTXEntLoss"><code class="docutils literal notranslate"><span class="pre">NTXEntLoss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.NTXEntLoss.forward"><code class="docutils literal notranslate"><span class="pre">NTXEntLoss.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.NegativeCosineSimilarity"><code class="docutils literal notranslate"><span class="pre">NegativeCosineSimilarity</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.NegativeCosineSimilarity.forward"><code class="docutils literal notranslate"><span class="pre">NegativeCosineSimilarity.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.VICRegLoss"><code class="docutils literal notranslate"><span class="pre">VICRegLoss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.VICRegLoss.forward"><code class="docutils literal notranslate"><span class="pre">VICRegLoss.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.iBOTPatchLoss"><code class="docutils literal notranslate"><span class="pre">iBOTPatchLoss</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.iBOTPatchLoss.forward"><code class="docutils literal notranslate"><span class="pre">iBOTPatchLoss.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.iBOTPatchLoss.sinkhorn_knopp_teacher"><code class="docutils literal notranslate"><span class="pre">iBOTPatchLoss.sinkhorn_knopp_teacher()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.mae"><code class="docutils literal notranslate"><span class="pre">mae()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.off_diagonal"><code class="docutils literal notranslate"><span class="pre">off_diagonal()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.losses.sinkhorn_knopp"><code class="docutils literal notranslate"><span class="pre">sinkhorn_knopp()</span></code></a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By stable-pretraining team
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024, stable-pretraining team.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>