
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/imbalance_supervised_learning.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_imbalance_supervised_learning.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_imbalance_supervised_learning.py:


This example demonstrates how to use stable-SSL to train a supervised model on CIFAR10
with class imbalance.

.. GENERATED FROM PYTHON SOURCE LINES 5-130

.. code-block:: Python


    import hydra
    from omegaconf import DictConfig
    import torch
    import torch.nn.functional as F
    import torchvision
    from torchvision import transforms

    import stable_ssl as ssl
    from stable_ssl.model.supervised import Supervised
    import numpy as np


    class MyCustomSupervised(Supervised):
        def initialize_train_loader(self):
            transform = transforms.Compose(
                [
                    transforms.ToTensor(),
                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
                ]
            )
            trainset = torchvision.datasets.CIFAR10(
                root=self.config.root, train=True, download=True, transform=transform
            )
            distribution = np.exp(np.linspace(0, self.config.distribution, 10))
            distribution /= np.sum(distribution)
            trainset = ssl.data.resample_classes(trainset, distribution)
            trainloader = torch.utils.data.DataLoader(
                trainset,
                batch_size=self.config.optim.batch_size,
                shuffle=True,
                num_workers=2,
                drop_last=True,
            )
            return trainloader

        def initialize_test_loader(self):
            transform = transforms.Compose(
                [
                    transforms.ToTensor(),
                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
                ]
            )
            testset = torchvision.datasets.CIFAR10(
                root=self.config.root, train=False, download=True, transform=transform
            )
            testloader = torch.utils.data.DataLoader(
                testset, batch_size=self.config.optim.batch_size, num_workers=2
            )
            return testloader

        def initialize_modules(self):
            self.model = ssl.utils.nn.resnet9()

        def forward(self, x):
            return self.model(x)

        def compute_loss(self):
            """The computer loss is called during training on each mini-batch
            stable-SSL automatically stores the output of the data loader as `self.data`
            which you can access directly within that function"""
            preds = self.forward(self.data[0])
            print(self.data[1][:4])
            self.log(
                {"train/step/acc1": self.metrics["train/step/acc1"](preds, self.data[1])},
                commit=False,
            )
            return F.cross_entropy(preds, self.data[1])


    @hydra.main(version_base=None)
    def main(cfg: DictConfig):

        args = ssl.get_args(cfg)

        print("--- Arguments ---")
        print(args)

        # while we provide a lot of config parameters (e.g. `optim.batch_size`), you can
        # also pass arguments directly when calling your model, they will be logged and
        #  accessible from within the model as `self.config.root` (in this example)
        trainer = MyCustomSupervised(args, root="~/data")
        trainer()


    def visualization():
        import matplotlib.pyplot as plt
        from matplotlib import colormaps
        import seaborn

        seaborn.set(font_scale=2)

        cmap = colormaps.get_cmap("cool")

        configs, values = ssl.reader.jsonl_project("experiment_llm")
        distris = {j: i for i, j in enumerate(np.unique(configs["distribution"]))}
        print(distris)
        fig, axs = plt.subplots(1, 1, sharey="all", sharex="all", figsize=(10, 7))

        for (_, c), v in zip(configs.iterrows(), values):
            if c["distribution"] > 0.01:
                continue
            axs.plot(
                v[-1]["eval/epoch/acc1_by_class"],
                c=cmap(np.sqrt(np.sqrt(c["optim.weight_decay"] / 10))),
                linewidth=3,
            )
            print(
                "(",
                c["optim.weight_decay"],
                ",",
                np.round(100 * np.array(v[-1]["eval/epoch/acc1_by_class"]), 2),
                ")",
            )

        plt.ylabel("test accuracy")
        plt.xlabel("class index")
        plt.tight_layout()
        plt.savefig("imbalance_classification.png")
        plt.close()


    if __name__ == "__main__":
        main()
        visualization()


.. _sphx_glr_download_auto_examples_imbalance_supervised_learning.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: imbalance_supervised_learning.ipynb <imbalance_supervised_learning.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: imbalance_supervised_learning.py <imbalance_supervised_learning.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: imbalance_supervised_learning.zip <imbalance_supervised_learning.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
