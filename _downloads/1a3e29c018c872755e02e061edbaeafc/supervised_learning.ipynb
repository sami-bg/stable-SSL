{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Supervised Learning Example\n\nThis example demonstrates how to train models using supervised learning\nwith stable-pretraining, including support for various datasets like\nImageNet-10, ImageNet-100, and ImageNet-1k.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import hydra\n\n\ndef get_data_loaders(cfg):\n    from stable_pretraining.data import transforms\n    import stable_pretraining as spt\n    import torch\n    import numpy as np\n\n    mean = [0.485, 0.456, 0.406]\n    std = [0.229, 0.224, 0.225]\n    if cfg.get(\"dataset_name\", \"inet100\") == \"inet10\":\n        path = \"frgfm/imagenette\"\n        name = \"full_size\"\n    elif cfg.get(\"dataset_name\", \"inet100\") == \"inet100\":\n        path = \"ilee0022/ImageNet100\"\n        name = None\n    elif cfg.get(\"dataset_name\", \"inet1k\") == \"inet1k\":\n        path = \"ILSVRC/imagenet-1k\"\n        name = None\n\n    train_transform = transforms.Compose(\n        transforms.RGB(),\n        transforms.RandomResizedCrop((224, 224)),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.ColorJitter(\n            brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1, p=0.8\n        ),\n        transforms.RandomGrayscale(p=0.2),\n        transforms.GaussianBlur(kernel_size=(5, 5), p=1.0),\n        transforms.ToImage(mean=mean, std=std),\n    )\n    train_dataset = spt.data.HFDataset(\n        path=path, name=name, split=\"train\", transform=train_transform\n    )\n    num_classes = int(np.max(train_dataset.dataset[\"label\"]) + 1)\n\n    train_loader = torch.utils.data.DataLoader(\n        dataset=train_dataset,\n        sampler=spt.data.sampler.RepeatedRandomSampler(\n            train_dataset, n_views=cfg.get(\"n_views\", 1)\n        ),\n        batch_size=cfg.get(\"batch_size\", 256),\n        num_workers=cfg.get(\"num_workers\", 10),\n        drop_last=True,\n        pin_memory=True,\n    )\n    val_transform = transforms.Compose(\n        transforms.RGB(),\n        transforms.Resize((256, 256)),\n        transforms.CenterCrop((224, 224)),\n        transforms.ToImage(mean=mean, std=std),\n    )\n    val_loader = torch.utils.data.DataLoader(\n        dataset=spt.data.HFDataset(\n            path=path, name=name, split=\"validation\", transform=val_transform\n        ),\n        batch_size=256,\n        num_workers=cfg.get(\"num_workers\", 10),\n    )\n    return train_loader, val_loader, num_classes\n\n\n@hydra.main(version_base=\"1.3\")\ndef main(cfg: dict):\n    import stable_pretraining as spt\n    import torch\n    from transformers import AutoModelForImageClassification, AutoConfig\n    import lightning as pl\n    import torchmetrics\n    from lightning.pytorch.loggers import WandbLogger\n    from functools import partial\n\n    import torchvision\n\n    # without transform\n    train_loader, val_loader, num_classes = get_data_loaders(cfg)\n    data_module = spt.data.DataModule(train=train_loader, val=val_loader)\n\n    def forward(self, batch, stage):\n        batch[\"embedding\"] = self.backbone(batch[\"image\"])[\"logits\"]\n        batch[\"projector\"] = self.projector(batch[\"embedding\"])\n        if self.training:\n            loss = torch.nn.functional.cross_entropy(\n                batch[\"projector\"],\n                batch[\"label\"],\n                label_smoothing=cfg.get(\"label_smoothing\", 0),\n            )\n            batch[\"loss\"] = loss\n        return batch\n\n    config = AutoConfig.from_pretrained(cfg.get(\"backbone\", \"microsoft/resnet-18\"))\n    backbone = AutoModelForImageClassification.from_config(config)\n    backbone = spt.backbone.utils.set_embedding_dim(\n        backbone, cfg.get(\"embedding_dim\", 2048)\n    )\n    if cfg.get(\"projector_arch\", \"linear\") == \"linear\":\n        projector = torch.nn.Linear(\n            cfg.get(\"embedding_dim\", 2048), cfg.get(\"projector_dim\", 128)\n        )\n    elif cfg.get(\"projector_arch\", \"linear\") == \"identity\":\n        projector = torch.nn.Identity()\n        cfg[\"projector_dim\"] = cfg.get(\"embedding_dim\", 2048)\n    else:\n        projector = torchvision.ops.MLP(\n            cfg.get(\"embedding_dim\", 2048),\n            hidden_channels=[2048, 2048, cfg.get(\"projector_dim\", 128)],\n            norm_layer=torch.nn.BatchNorm1d,\n        )\n    module = spt.Module(\n        backbone=backbone,\n        projector=projector,\n        forward=forward,\n        hparams=cfg,\n        optim={\n            \"optimizer\": partial(\n                torch.optim.AdamW,\n                lr=cfg.get(\"lr\", 1e-3),\n                weight_decay=cfg.get(\"weight_decay\", 1e-3),\n            ),\n            \"scheduler\": \"LinearWarmupCosineAnnealing\",\n        },\n    )\n    linear_probe = spt.callbacks.OnlineProbe(\n        module,\n        name=\"linear_probe\",\n        input=\"embedding\",\n        target=\"label\",\n        probe=torch.nn.Linear(cfg.get(\"embedding_dim\", 2048), num_classes),\n        loss=torch.nn.CrossEntropyLoss(),\n        metrics=torchmetrics.classification.MulticlassAccuracy(num_classes),\n    )\n    linear_probe_proj = spt.callbacks.OnlineProbe(\n        module,\n        name=\"linear_probe_proj\",\n        input=\"projector\",\n        target=\"label\",\n        probe=torch.nn.Linear(cfg.get(\"projector_dim\", 128), num_classes),\n        loss=torch.nn.CrossEntropyLoss(),\n        metrics=torchmetrics.classification.MulticlassAccuracy(num_classes),\n    )\n    lr_monitor = pl.pytorch.callbacks.LearningRateMonitor(\n        logging_interval=\"step\", log_momentum=True, log_weight_decay=True\n    )\n    logger = WandbLogger(project=cfg.get(\"wandb_project\", \"supervised_learning\"))\n    trainer = pl.Trainer(\n        max_epochs=cfg.get(\"max_epochs\", 100),\n        num_sanity_val_steps=1,\n        callbacks=[lr_monitor, linear_probe, linear_probe_proj],\n        precision=\"16-mixed\",\n        logger=logger,\n        sync_batchnorm=True,\n        enable_checkpointing=False,\n    )\n    manager = spt.Manager(trainer=trainer, module=module, data=data_module)\n    manager()\n    manager.validate()\n\n\nif __name__ == \"__main__\":\n    \"\"\"Examples to run:\n    HYDRA_FULL_ERROR=1 python supervised_learning.py ++embedding_dim=2048 ++projector_dim=256 ++projector_arch=linear ++dataset_name=inet100 ++max_epochs=50 ++batch_size=256 ++backbone=microsoft/resnet-50\n    HYDRA_FULL_ERROR=1 python supervised_learning.py ++embedding_dim=2048 ++projector_dim=256 ++projector_arch=MLP ++dataset_name=inet100 ++max_epochs=50 ++batch_size=256 ++backbone=microsoft/resnet-18\n    \"\"\"\n    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}